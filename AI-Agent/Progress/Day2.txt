# Roadmap for Day 2: Sitemap & Seed-List Crawling
# Overview: Build on Day 1 by adding sitemap parsing to discover URLs, then create a script to crawl a limited set of those URLs using your existing scraper, saving outputs as JSON files organized by domain.

# High-Level Steps:
# 1. Checkout a new branch for Day 2.
# 2. Create the sitemap parser module.
# 3. Build the crawl_seed script to fetch from sitemap and scrape N pages.
# 4. Handle file saving in a structured directory.
# 5. Test with a real site (e.g., a news site with public sitemap like BBC or CNN).
# 6. Update README with new run instructions.
# 7. Commit the changes.

# Detailed Step-by-Step Guide (Run these in your terminal, but type code manually in editor)

# Step 1: Branch Management
# Ensure you're on main or previous branch, then create new feature branch
git checkout main  # Or feature/day-01 if not merged
git checkout -b feature/day-02

# Step 2: Create sitemap.py
# Create the file in src/scraper
touch src/scraper/sitemap.py
# Open in editor: code src/scraper/sitemap.py (or your editor command)
# Inside: Define a function parse_sitemap(sitemap_url) that:
# - Fetches the sitemap XML using requests (with User-Agent).
# - Parses XML with BeautifulSoup (use 'xml' parser).
# - Extracts <loc> tags for URLs (handle nested sitemaps if needed, but keep simple for now).
# - Returns a list of URLs.
# Tip: Import requests, bs4, and maybe logging for errors.

# Step 3: Create crawl_seed.py
# Create the script in src/scraper
touch src/scraper/crawl_seed.py
# Open in editor: code src/scraper/crawl_seed.py
# Inside: Use if __name__ == "__main__":
# - Take args: sitemap_url (required), N (optional, default 5 for first N pages).
# - Use argparse for better CLI (import argparse).
# - Call parse_sitemap to get URL list.
# - For each of first N URLs: Run the scraping logic (import and call from simple_scraper.py, or reuse code).
# - For each scraped dict: Extract domain from URL (use urllib.parse.urlparse).
# - Create dir: mkdir -p data/raw/<domain>
# - Save as JSON: Use json.dump to file like data/raw/<domain>/<slug_or_id>.json (make filename from URL path).
# - Handle errors gracefully (e.g., skip bad URLs).

# Step 4: Directory Setup
# Create the data directory structure
mkdir -p data/raw

# Step 5: Testing
# Run the crawl_seed script with a test sitemap
# Example: Find a site's sitemap (e.g., https://www.bbc.com/sitemaps/https-sitemap-uk-archive-1.xml)
python src/scraper/crawl_seed.py https://example.com/sitemap.xml --N 3
# Check data/raw/<domain>/ for JSON files.
# Verify contents: cat data/raw/example.com/some-page.json

# Step 6: Update README.md
# Open README: code README.md
# Add section: ## Day 2: Sitemap Crawling
# Instructions:
# - To parse sitemap: Import and call parse_sitemap(url)
# - To run seed crawl: python src/scraper/crawl_seed.py <sitemap_url> [--N <num_pages>]
# - Outputs: JSON files in data/raw/<domain>/

# Step 7: Commit
git add src/scraper/sitemap.py src/scraper/crawl_seed.py README.md data/  # Add data if you want examples, else .gitignore it
git commit -m "feat(day-02): sitemap parser + seed crawl; save JSON snapshots"

# Tips for Learning:
# - Research sitemap XML structure (standard <urlset><url><loc>).
# - Reuse code from simple_scraper.py (e.g., make it a function to import).
# - Handle rate limiting: Add time.sleep(1) between requests.
# - For domains: From urlparse(url).netloc
# - Filenames: Sanitize URL path (e.g., replace / with _).
# - If sitemap is gzipped, handle with requests and gzip.
# Once tested, merge to main if ready.

















#######################################################
# ğŸ§  MY WEB SCRAPING & SITEMAP NOTES â€” COPY & PASTE ğŸ§ 
#######################################################

âœ… COVERED BY ME:
------------------------------------------------------
1ï¸âƒ£ <loc> TAGS ONLY EXIST IN XML SITEMAPS â€” NOT IN HTML PAGES.
   â†’ Example: https://example.com/sitemap.xml âœ…
   â†’ Not in: https://example.com/ âŒ

2ï¸âƒ£ NORMAL WEB PAGES (like CNN or CodersDaddy homepage) RETURN HTML.
   â†’ They contain <div>, <h1>, <p> â€” NOT <loc> tags.

3ï¸âƒ£ TO GET <loc> TAGS â†’ YOU MUST REQUEST A REAL SITEMAP.XML URL.
   â†’ Always verify by opening the URL in browser first!

4ï¸âƒ£ CODERSDADDYâ€™S â€œsitemap.xmlâ€ IS NOT STANDARD XML â€” ITâ€™S PLAIN TEXT!
   â†’ No <loc> tags â†’ soup.find_all('loc') returns [] âŒ
   â†’ Must parse line-by-line â†’ if line.startswith('http') âœ…

5ï¸âƒ£ ALWAYS STRIP WHITESPACE FROM URLS â†’ .strip() is your friend!
   â†’ "url  " â†’ breaks everything ğŸ˜…

6ï¸âƒ£ return print(urls) â†’ RETURNS None âŒ
   â†’ Do: print(urls) THEN return urls âœ…

7ï¸âƒ£ XML PARSER WONâ€™T CREATE <loc> TAGS IF THEY DONâ€™T EXIST.
   â†’ Garbage in â†’ garbage out. Always check raw content first!

â— WHAT I MISSED / SHOULD ADD:
-------------------------------------------------------
ğŸ”¹ NOT ALL â€œsitemap.xmlâ€ FILES ARE THE SAME:
   - âœ… Standard XML (with <url><loc>...</loc></url>)
   - âš ï¸ Plain Text (CodersDaddy style â€” raw URLs)
   - ğŸ”„ Sitemap Index (contains <sitemap><loc>...</loc></sitemap> â†’ points to child sitemaps)

ğŸ”¹ ALWAYS CHECK Content-Type HEADER or RAW TEXT FIRST:
   â†’ print(response.text[:500]) â€” see what youâ€™re REALLY getting!

ğŸ”¹ NAMESPACE AWARENESS (for strict XML sitemaps):
   â†’ Some sitemaps use xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
   â†’ BeautifulSoup sometimes needs help â†’ but often .find_all('loc') still works!

ğŸ”¹ ROBOTS.TXT IS YOUR BEST FRIEND:
   â†’ https://example.com/robots.txt â†’ look for Sitemap: ... ğŸ—ºï¸
   â†’ Often reveals the TRUE sitemap URL!

ğŸ”¹ ERROR HANDLING IS KEY:
   â†’ Always wrap requests in try/except
   â†’ Always check response.status_code
   â†’ Use response.raise_for_status()

ğŸ”¹ UNIVERSAL PARSER IDEA (BONUS):
   â†’ Try XML â†’ if no <loc> tags â†’ fallback to text parsing â†’ profit! ğŸ’°

ğŸ’¡ PRO TIPS I LEARNED:
-------------------------------------------------------
âœ¨ ALWAYS INSPECT RAW CONTENT FIRST â†’ Ctrl+U or print(response.text)
âœ¨ USE .strip() ON URLS â†’ avoid sneaky whitespace bugs ğŸ›
âœ¨ return AND print SEPARATELY â†’ donâ€™t mix them!
âœ¨ TEST WITH print() INSIDE FUNCTION â†’ then return the actual data
âœ¨ CODERSDADDY TAUGHT ME: Real-world sitemaps are messy â€” adapt your parser!

ğŸš€ NEXT STEPS:
-------------------------------------------------------
â†’ Build a universal sitemap parser (XML + text + index)
â†’ Save URLs to CSV/JSON
â†’ Crawl extracted URLs for content
â†’ Add concurrency (async/threads) for speed
â†’ Respect robots.txt + add delays (be ethical scraper!)

ğŸ“š RESOURCES:
-------------------------------------------------------
- Sitemap Protocol: https://www.sitemaps.org/protocol.html
- robots.txt Spec: https://developers.google.com/search/docs/crawling-indexing/robots/intro
- BeautifulSoup Docs: https://www.crummy.com/software/BeautifulSoup/bs4/doc/

âœ… YOUâ€™RE DOING AWESOME â€” KEEP EXPERIMENTING!