# Roadmap for Day 2: Sitemap & Seed-List Crawling
# Overview: Build on Day 1 by adding sitemap parsing to discover URLs, then create a script to crawl a limited set of those URLs using your existing scraper, saving outputs as JSON files organized by domain.

# High-Level Steps:
# 1. Checkout a new branch for Day 2.
# 2. Create the sitemap parser module.
# 3. Build the crawl_seed script to fetch from sitemap and scrape N pages.
# 4. Handle file saving in a structured directory.
# 5. Test with a real site (e.g., a news site with public sitemap like BBC or CNN).
# 6. Update README with new run instructions.
# 7. Commit the changes.

# Detailed Step-by-Step Guide (Run these in your terminal, but type code manually in editor)

# Step 1: Branch Management
# Ensure you're on main or previous branch, then create new feature branch
git checkout main  # Or feature/day-01 if not merged
git checkout -b feature/day-02

# Step 2: Create sitemap.py
# Create the file in src/scraper
touch src/scraper/sitemap.py
# Open in editor: code src/scraper/sitemap.py (or your editor command)
# Inside: Define a function parse_sitemap(sitemap_url) that:
# - Fetches the sitemap XML using requests (with User-Agent).
# - Parses XML with BeautifulSoup (use 'xml' parser).
# - Extracts <loc> tags for URLs (handle nested sitemaps if needed, but keep simple for now).
# - Returns a list of URLs.
# Tip: Import requests, bs4, and maybe logging for errors.

# Step 3: Create crawl_seed.py
# Create the script in src/scraper
touch src/scraper/crawl_seed.py
# Open in editor: code src/scraper/crawl_seed.py
# Inside: Use if __name__ == "__main__":
# - Take args: sitemap_url (required), N (optional, default 5 for first N pages).
# - Use argparse for better CLI (import argparse).
# - Call parse_sitemap to get URL list.
# - For each of first N URLs: Run the scraping logic (import and call from simple_scraper.py, or reuse code).
# - For each scraped dict: Extract domain from URL (use urllib.parse.urlparse).
# - Create dir: mkdir -p data/raw/<domain>
# - Save as JSON: Use json.dump to file like data/raw/<domain>/<slug_or_id>.json (make filename from URL path).
# - Handle errors gracefully (e.g., skip bad URLs).

# Step 4: Directory Setup
# Create the data directory structure
mkdir -p data/raw

# Step 5: Testing
# Run the crawl_seed script with a test sitemap
# Example: Find a site's sitemap (e.g., https://www.bbc.com/sitemaps/https-sitemap-uk-archive-1.xml)
python src/scraper/crawl_seed.py https://example.com/sitemap.xml --N 3
# Check data/raw/<domain>/ for JSON files.
# Verify contents: cat data/raw/example.com/some-page.json

# Step 6: Update README.md
# Open README: code README.md
# Add section: ## Day 2: Sitemap Crawling
# Instructions:
# - To parse sitemap: Import and call parse_sitemap(url)
# - To run seed crawl: python src/scraper/crawl_seed.py <sitemap_url> [--N <num_pages>]
# - Outputs: JSON files in data/raw/<domain>/

# Step 7: Commit
git add src/scraper/sitemap.py src/scraper/crawl_seed.py README.md data/  # Add data if you want examples, else .gitignore it
git commit -m "feat(day-02): sitemap parser + seed crawl; save JSON snapshots"

# Tips for Learning:
# - Research sitemap XML structure (standard <urlset><url><loc>).
# - Reuse code from simple_scraper.py (e.g., make it a function to import).
# - Handle rate limiting: Add time.sleep(1) between requests.
# - For domains: From urlparse(url).netloc
# - Filenames: Sanitize URL path (e.g., replace / with _).
# - If sitemap is gzipped, handle with requests and gzip.
# Once tested, merge to main if ready.

















#######################################################
# 🧠 MY WEB SCRAPING & SITEMAP NOTES — COPY & PASTE 🧠
#######################################################

✅ COVERED BY ME:
------------------------------------------------------
1️⃣ <loc> TAGS ONLY EXIST IN XML SITEMAPS — NOT IN HTML PAGES.
   → Example: https://example.com/sitemap.xml ✅
   → Not in: https://example.com/ ❌

2️⃣ NORMAL WEB PAGES (like CNN or CodersDaddy homepage) RETURN HTML.
   → They contain <div>, <h1>, <p> — NOT <loc> tags.

3️⃣ TO GET <loc> TAGS → YOU MUST REQUEST A REAL SITEMAP.XML URL.
   → Always verify by opening the URL in browser first!

4️⃣ CODERSDADDY’S “sitemap.xml” IS NOT STANDARD XML — IT’S PLAIN TEXT!
   → No <loc> tags → soup.find_all('loc') returns [] ❌
   → Must parse line-by-line → if line.startswith('http') ✅

5️⃣ ALWAYS STRIP WHITESPACE FROM URLS → .strip() is your friend!
   → "url  " → breaks everything 😅

6️⃣ return print(urls) → RETURNS None ❌
   → Do: print(urls) THEN return urls ✅

7️⃣ XML PARSER WON’T CREATE <loc> TAGS IF THEY DON’T EXIST.
   → Garbage in → garbage out. Always check raw content first!

❗ WHAT I MISSED / SHOULD ADD:
-------------------------------------------------------
🔹 NOT ALL “sitemap.xml” FILES ARE THE SAME:
   - ✅ Standard XML (with <url><loc>...</loc></url>)
   - ⚠️ Plain Text (CodersDaddy style — raw URLs)
   - 🔄 Sitemap Index (contains <sitemap><loc>...</loc></sitemap> → points to child sitemaps)

🔹 ALWAYS CHECK Content-Type HEADER or RAW TEXT FIRST:
   → print(response.text[:500]) — see what you’re REALLY getting!

🔹 NAMESPACE AWARENESS (for strict XML sitemaps):
   → Some sitemaps use xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
   → BeautifulSoup sometimes needs help → but often .find_all('loc') still works!

🔹 ROBOTS.TXT IS YOUR BEST FRIEND:
   → https://example.com/robots.txt → look for Sitemap: ... 🗺️
   → Often reveals the TRUE sitemap URL!

🔹 ERROR HANDLING IS KEY:
   → Always wrap requests in try/except
   → Always check response.status_code
   → Use response.raise_for_status()

🔹 UNIVERSAL PARSER IDEA (BONUS):
   → Try XML → if no <loc> tags → fallback to text parsing → profit! 💰

💡 PRO TIPS I LEARNED:
-------------------------------------------------------
✨ ALWAYS INSPECT RAW CONTENT FIRST → Ctrl+U or print(response.text)
✨ USE .strip() ON URLS → avoid sneaky whitespace bugs 🐛
✨ return AND print SEPARATELY → don’t mix them!
✨ TEST WITH print() INSIDE FUNCTION → then return the actual data
✨ CODERSDADDY TAUGHT ME: Real-world sitemaps are messy — adapt your parser!

🚀 NEXT STEPS:
-------------------------------------------------------
→ Build a universal sitemap parser (XML + text + index)
→ Save URLs to CSV/JSON
→ Crawl extracted URLs for content
→ Add concurrency (async/threads) for speed
→ Respect robots.txt + add delays (be ethical scraper!)

📚 RESOURCES:
-------------------------------------------------------
- Sitemap Protocol: https://www.sitemaps.org/protocol.html
- robots.txt Spec: https://developers.google.com/search/docs/crawling-indexing/robots/intro
- BeautifulSoup Docs: https://www.crummy.com/software/BeautifulSoup/bs4/doc/

✅ YOU’RE DOING AWESOME — KEEP EXPERIMENTING!