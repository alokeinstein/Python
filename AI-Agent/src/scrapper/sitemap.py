# Open in editor: code src/scraper/sitemap.py (or your editor command)
# Inside: Define a function parse_sitemap(sitemap_url) that:
# - Fetches the sitemap XML using requests (with User-Agent).
# - Parses XML with BeautifulSoup (use 'xml' parser).
# - Extracts <loc> tags for URLs (handle nested sitemaps if needed, but keep simple for now).
# - Returns a list of URLs.


# bs4: Beautiful Soup is a Python library for pulling data out of HTML and XML files.
# lxml: It is a Python library that allows us to handle XML and HTML files.


import requests
from bs4 import BeautifulSoup 

def parse_sitemap(sitemap_url):
     sitemap_url = sitemap_url.strip()
     headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
     
     try:
        response = requests.get(sitemap_url, headers= headers)
        response.raise_for_status()
     except Exception as e:
        print(f"Failed to fetch: {e}")
        return []
        
     soup = BeautifulSoup(response.content, 'xml')
     # print(soup)
     loc_tag = soup.find_all('loc')
     urls = [tag.get_text().strip() for tag in loc_tag]
     return urls
     

"""This line calls the function but does not print the urls in the console, so other ways can be, you can return a print() or use use function copy method and print the copied function. """
# parse_sitemap("https://www.codersdaddy.com/sitemap.xml")
result = parse_sitemap("https://www.codersdaddy.com/sitemap.xml")
print(result)

##NEW THINGS I LEARNT
"""
1. <loc> tag is not present in the HTML file , it is only present in XML file.
2. normal url like "https://www.codersdaddy.com" will not give you xml file 
3. even if you try to extract the content from the xml parser. You have to give me sitemap.xml url 

4. ğŸ“Œ Exception: A malicious or misconfigured site could put a <loc> tag in HTML â€” but itâ€™s meaningless there and not standard.
"""














#######################################################
# ğŸ§  MY WEB SCRAPING & SITEMAP NOTES â€” COPY & PASTE ğŸ§ 
#######################################################

âœ… COVERED BY ME:
-------------------------------------------------------
1ï¸âƒ£ <loc> TAGS ONLY EXIST IN XML SITEMAPS â€” NOT IN HTML PAGES.
   â†’ Example: https://example.com/sitemap.xml âœ…
   â†’ Not in: https://example.com/ âŒ

2ï¸âƒ£ NORMAL WEB PAGES (like CNN or CodersDaddy homepage) RETURN HTML.
   â†’ They contain <div>, <h1>, <p> â€” NOT <loc> tags.

3ï¸âƒ£ TO GET <loc> TAGS â†’ YOU MUST REQUEST A REAL SITEMAP.XML URL.
   â†’ Always verify by opening the URL in browser first!

4ï¸âƒ£ CODERSDADDYâ€™S â€œsitemap.xmlâ€ IS NOT STANDARD XML â€” ITâ€™S PLAIN TEXT!
   â†’ No <loc> tags â†’ soup.find_all('loc') returns [] âŒ
   â†’ Must parse line-by-line â†’ if line.startswith('http') âœ…

5ï¸âƒ£ ALWAYS STRIP WHITESPACE FROM URLS â†’ .strip() is your friend!
   â†’ "url  " â†’ breaks everything ğŸ˜…

6ï¸âƒ£ return print(urls) â†’ RETURNS None âŒ
   â†’ Do: print(urls) THEN return urls âœ…

7ï¸âƒ£ XML PARSER WONâ€™T CREATE <loc> TAGS IF THEY DONâ€™T EXIST.
   â†’ Garbage in â†’ garbage out. Always check raw content first!

â— WHAT I MISSED / SHOULD ADD:
-------------------------------------------------------
ğŸ”¹ NOT ALL â€œsitemap.xmlâ€ FILES ARE THE SAME:
   - âœ… Standard XML (with <url><loc>...</loc></url>)
   - âš ï¸ Plain Text (CodersDaddy style â€” raw URLs)
   - ğŸ”„ Sitemap Index (contains <sitemap><loc>...</loc></sitemap> â†’ points to child sitemaps)

ğŸ”¹ ALWAYS CHECK Content-Type HEADER or RAW TEXT FIRST:
   â†’ print(response.text[:500]) â€” see what youâ€™re REALLY getting!

ğŸ”¹ NAMESPACE AWARENESS (for strict XML sitemaps):
   â†’ Some sitemaps use xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
   â†’ BeautifulSoup sometimes needs help â†’ but often .find_all('loc') still works!

ğŸ”¹ ROBOTS.TXT IS YOUR BEST FRIEND:
   â†’ https://example.com/robots.txt â†’ look for Sitemap: ... ğŸ—ºï¸
   â†’ Often reveals the TRUE sitemap URL!

ğŸ”¹ ERROR HANDLING IS KEY:
   â†’ Always wrap requests in try/except
   â†’ Always check response.status_code
   â†’ Use response.raise_for_status()

ğŸ”¹ UNIVERSAL PARSER IDEA (BONUS):
   â†’ Try XML â†’ if no <loc> tags â†’ fallback to text parsing â†’ profit! ğŸ’°

ğŸ’¡ PRO TIPS I LEARNED:
-------------------------------------------------------
âœ¨ ALWAYS INSPECT RAW CONTENT FIRST â†’ Ctrl+U or print(response.text)
âœ¨ USE .strip() ON URLS â†’ avoid sneaky whitespace bugs ğŸ›
âœ¨ return AND print SEPARATELY â†’ donâ€™t mix them!
âœ¨ TEST WITH print() INSIDE FUNCTION â†’ then return the actual data
âœ¨ CODERSDADDY TAUGHT ME: Real-world sitemaps are messy â€” adapt your parser!

ğŸš€ NEXT STEPS:
-------------------------------------------------------
â†’ Build a universal sitemap parser (XML + text + index)
â†’ Save URLs to CSV/JSON
â†’ Crawl extracted URLs for content
â†’ Add concurrency (async/threads) for speed
â†’ Respect robots.txt + add delays (be ethical scraper!)

ğŸ“š RESOURCES:
-------------------------------------------------------
- Sitemap Protocol: https://www.sitemaps.org/protocol.html
- robots.txt Spec: https://developers.google.com/search/docs/crawling-indexing/robots/intro
- BeautifulSoup Docs: https://www.crummy.com/software/BeautifulSoup/bs4/doc/

âœ… YOUâ€™RE DOING AWESOME â€” KEEP EXPERIMENTING!